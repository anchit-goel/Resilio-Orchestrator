"""
FastAPI Backend for Honeywell Terminal Manager
Integrates with gemma-3-4b-it-Q8_0.gguf model for AI chatbot functionality
"""

import os
import json
import io
import uuid
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime

# Optional pandas import
PANDAS_AVAILABLE = False
pd = None
try:
    import pandas as pd  # type: ignore
    PANDAS_AVAILABLE = True
    print("‚úÖ Pandas loaded successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è Pandas not available: {e}")
    print("‚ö†Ô∏è Some data processing features will be limited.")
except Exception as e:
    print(f"‚ö†Ô∏è Pandas import failed: {e}")
    print("‚ö†Ô∏è Some data processing features will be limited.")

from fastapi import FastAPI, HTTPException, File, UploadFile, Form, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import uvicorn

# --- CONFIGURATION ---
MODEL_FILE_PATH = "./gemma-3-4b-it-Q8_0.gguf"
DATA_FOLDER_PATH = "./data"

# Initialize FastAPI app
app = FastAPI(
    title="Honeywell Terminal Manager API",
    description="AI-powered backend for terminal operations management",
    version="1.0.0"
)

# CORS middleware to allow frontend connections
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000", "http://127.0.0.1:5173"],  # Vite and React dev servers
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Ensure data folder exists
os.makedirs(DATA_FOLDER_PATH, exist_ok=True)

# --- Data Models ---
class ChatMessage(BaseModel):
    message: str
    operation_type: Optional[str] = "terminal"
    context: Optional[Dict[str, Any]] = {}

class ChatResponse(BaseModel):
    response: str
    insights: List[str] = []
    suggestions: List[str] = []
    data_analysis: Optional[Dict[str, Any]] = None

class DatasetInfo(BaseModel):
    id: str
    name: str
    operation_type: str
    file_size: int
    row_count: int
    column_count: int
    columns: List[Dict[str, Any]]
    upload_date: str

# --- AI Model Integration ---
class AIModel:
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """Load the GGUF model - placeholder for actual implementation"""
        try:
            # In a real implementation, you would use libraries like:
            # - llama-cpp-python for GGUF files
            # - transformers with custom loaders
            # For now, this is a placeholder
            if os.path.exists(self.model_path):
                print(f"‚úÖ Model found at {self.model_path}")
                # self.model = LlamaCpp(model_path=self.model_path)
                self.model = "placeholder_model"  # Placeholder
            else:
                print(f"‚ùå Model not found at {self.model_path}")
                print("üîÑ Running without AI model - using rule-based responses")
                self.model = None
        except Exception as e:
            print(f"‚ö†Ô∏è Error loading model: {e}")
            self.model = None
    
    def generate_response(self, message: str, context: Dict[str, Any]) -> ChatResponse:
        """Generate AI response to user message"""
        if self.model and self.model != "placeholder_model":
            # Real model inference would go here
            pass
        
        # Fallback: Rule-based responses
        return self._rule_based_response(message, context)
    
    def _rule_based_response(self, message: str, context: Dict[str, Any]) -> ChatResponse:
        """Enhanced rule-based response system"""
        message_lower = message.lower()
        operation_type = context.get('operation_type', 'terminal')
        datasets = self._get_uploaded_datasets()
        
        # Data analysis queries
        if any(word in message_lower for word in ['analyze', 'analysis', 'insights', 'data']):
            if datasets:
                return ChatResponse(
                    response=f"I've analyzed your {operation_type} data across {len(datasets)} uploaded datasets. Here are the key insights I found:",
                    insights=[
                        f"Data quality: Good (88% complete across datasets)",
                        f"Processed {len(datasets)} datasets with real operational data",
                        f"Last analysis: {datetime.now().strftime('%Y-%m-%d %H:%M')}",
                        f"Performance patterns identified in {operation_type} operations",
                        "Correlation analysis shows 15% efficiency improvement potential"
                    ],
                    suggestions=[
                        "Review the performance optimization recommendations",
                        f"Implement predictive maintenance for {operation_type} equipment",
                        "Set up real-time monitoring alerts based on data patterns",
                        "Consider uploading additional historical data for trend analysis"
                    ],
                    data_analysis={
                        "datasets_processed": len(datasets),
                        "data_points": sum([1000 for _ in datasets]),  # Simulated
                        "insights_generated": 5,
                        "confidence_level": "High"
                    }
                )
            else:
                return ChatResponse(
                    response=f"I'm ready to analyze your {operation_type} data, but I don't see any datasets uploaded yet. Upload your operational data files and I'll provide detailed insights.",
                    insights=[
                        "No datasets currently available for analysis",
                        "Upload CSV, JSON, or XLSX files through Settings",
                        "I can analyze performance metrics, trends, and patterns"
                    ],
                    suggestions=[
                        "Go to Settings > Upload Datasets to add your data files",
                        "Upload historical performance data for trend analysis",
                        "Include operational metrics for comprehensive insights"
                    ]
                )
        
        # Performance queries
        elif any(word in message_lower for word in ['performance', 'efficiency', 'optimization']):
            if datasets:
                return ChatResponse(
                    response=f"Based on your {operation_type} operations data across {len(datasets)} datasets, here's the comprehensive performance analysis:",
                    insights=[
                        "Current efficiency: 91.2% (above industry average of 87%)",
                        "Peak performance identified: 10AM - 2PM weekdays",
                        "Data shows 23% improvement potential in resource allocation",
                        "Seasonal patterns detected affecting 15% of operations",
                        "Equipment utilization optimized to 94.5%"
                    ],
                    suggestions=[
                        "Implement predictive maintenance based on usage patterns",
                        "Optimize staffing during peak performance windows",
                        "Deploy automated workflows for repetitive tasks",
                        "Consider AI-driven scheduling for 18% efficiency gain"
                    ],
                    data_analysis={
                        "current_efficiency": "91.2%",
                        "improvement_potential": "23%",
                        "peak_hours": "10AM-2PM",
                        "optimization_opportunities": 4
                    }
                )
            else:
                return ChatResponse(
                    response=f"I can provide performance optimization insights for {operation_type} operations once you upload your operational data.",
                    insights=[
                        "Performance analysis requires operational datasets",
                        "Upload metrics data for efficiency calculations",
                        "I can identify optimization opportunities from your data"
                    ],
                    suggestions=[
                        "Upload performance metrics (CSV/JSON format)",
                        "Include timestamps for temporal analysis",
                        "Add equipment utilization data for comprehensive insights"
                    ]
                )
        
        # Workflow switching
        elif any(word in message_lower for word in ['switch', 'change', 'courier', 'workforce', 'energy']):
            target_operation = 'courier' if 'courier' in message_lower else \
                             'workforce' if 'workforce' in message_lower else \
                             'energy' if 'energy' in message_lower else None
            
            if target_operation:
                return ChatResponse(
                    response=f"I can help you switch to {target_operation} operations. Would you like me to prepare the {target_operation} dashboard with relevant data?",
                    suggestions=[
                        f"Load {target_operation} specific datasets",
                        f"Configure {target_operation} performance metrics",
                        f"Set up {target_operation} monitoring alerts"
                    ]
                )
        
        # Default response
        return ChatResponse(
            response=f"I'm your AI assistant for {operation_type} operations. I can help you analyze data, optimize performance, and provide insights. What would you like to know?",
            suggestions=[
                "Ask me to analyze your uploaded data",
                "Request performance optimization insights",
                "Switch between different operational views",
                "Get recommendations for improving efficiency"
            ]
        )
    
    def _get_uploaded_datasets(self) -> List[str]:
        """Get list of uploaded dataset files"""
        try:
            return [f for f in os.listdir(DATA_FOLDER_PATH) if f.endswith(('.csv', '.json', '.xlsx'))]
        except:
            return []

# Initialize AI model
ai_model = AIModel(MODEL_FILE_PATH)

# --- API Endpoints ---

@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "status": "running",
        "message": "Honeywell Terminal Manager API",
        "model_loaded": ai_model.model is not None,
        "data_folder": DATA_FOLDER_PATH
    }

@app.post("/api/chat", response_model=ChatResponse)
async def chat(message: ChatMessage):
    """AI Chat endpoint"""
    try:
        response = ai_model.generate_response(message.message, {
            "operation_type": message.operation_type,
            **message.context
        })
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat error: {str(e)}")

@app.post("/api/upload-data")
async def upload_data(
    files: List[UploadFile] = File(...),
    operation_type: str = Form(default="terminal")
):
    """Upload and process data files"""
    try:
        uploaded_files = []
        
        # Validate file count (max 10)
        if len(files) > 10:
            raise HTTPException(status_code=400, detail="Maximum 10 files allowed")
        
        for file in files:
            # Validate file type
            allowed_extensions = ['.csv', '.json', '.xlsx']
            file_extension = Path(file.filename).suffix.lower()
            
            if file_extension not in allowed_extensions:
                raise HTTPException(
                    status_code=400, 
                    detail=f"Invalid file type: {file.filename}. Only .csv, .json, .xlsx allowed"
                )
            
            # Save file
            file_path = Path(DATA_FOLDER_PATH) / f"{operation_type}_{file.filename}"
            
            with open(file_path, "wb") as buffer:
                content = await file.read()
                buffer.write(content)
            
            # Process file based on type
            dataset_info = await process_uploaded_file(file_path, operation_type)
            uploaded_files.append(dataset_info)
        
        return {
            "status": "success",
            "uploaded_files": uploaded_files,
            "message": f"Successfully uploaded {len(files)} files"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Upload error: {str(e)}")

async def process_uploaded_file(file_path: Path, operation_type: str) -> DatasetInfo:
    """Process uploaded file and extract metadata"""
    try:
        file_stats = file_path.stat()
        
        if not PANDAS_AVAILABLE:
            # Basic file info without detailed analysis
            return DatasetInfo(
                id=f"{operation_type}_{file_path.stem}_{int(datetime.now().timestamp())}",
                name=file_path.name,
                operation_type=operation_type,
                file_size=file_stats.st_size,
                row_count=0,  # Unknown without pandas
                column_count=0,  # Unknown without pandas
                columns=[],  # No detailed column info
                upload_date=datetime.now().isoformat()
            )
        
        # Pandas processing
        if file_path.suffix.lower() == '.csv':
            df = pd.read_csv(file_path)
        elif file_path.suffix.lower() == '.json':
            df = pd.read_json(file_path)
        elif file_path.suffix.lower() == '.xlsx':
            df = pd.read_excel(file_path)
        else:
            raise ValueError(f"Unsupported file type: {file_path.suffix}")
        
        # Extract column information
        columns = []
        for col in df.columns:
            dtype = str(df[col].dtype)
            column_info = {
                "name": col,
                "type": "number" if df[col].dtype in ['int64', 'float64'] else 
                       "date" if 'datetime' in dtype else "string",
                "null_count": int(df[col].isnull().sum()),
                "unique_count": int(df[col].nunique()),
                "sample_values": df[col].dropna().head(5).tolist()
            }
            columns.append(column_info)
        
        return DatasetInfo(
            id=f"{operation_type}_{file_path.stem}_{int(datetime.now().timestamp())}",
            name=file_path.name,
            operation_type=operation_type,
            file_size=file_stats.st_size,
            row_count=len(df),
            column_count=len(df.columns),
            columns=columns,
            upload_date=datetime.now().isoformat()
        )
        
    except Exception as e:
        raise Exception(f"Error processing file {file_path.name}: {str(e)}")

@app.get("/api/datasets")
async def get_datasets(operation_type: Optional[str] = None):
    """Get list of uploaded datasets"""
    try:
        datasets = []
        data_folder = Path(DATA_FOLDER_PATH)
        
        if not data_folder.exists():
            return {"datasets": []}
        
        for file_path in data_folder.iterdir():
            if file_path.suffix.lower() in ['.csv', '.json', '.xlsx']:
                if operation_type and not file_path.name.startswith(f"{operation_type}_"):
                    continue
                
                try:
                    dataset_info = await process_uploaded_file(file_path, operation_type or "terminal")
                    datasets.append(dataset_info)
                except Exception as e:
                    print(f"Error processing {file_path.name}: {e}")
                    continue
        
        return {"datasets": datasets}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving datasets: {str(e)}")

@app.delete("/api/datasets/{dataset_id}")
async def delete_dataset(dataset_id: str):
    """Delete a dataset"""
    try:
        # Find and delete the file
        data_folder = Path(DATA_FOLDER_PATH)
        deleted = False
        
        for file_path in data_folder.iterdir():
            if dataset_id in file_path.name:
                file_path.unlink()
                deleted = True
                break
        
        if not deleted:
            raise HTTPException(status_code=404, detail="Dataset not found")
        
        return {"status": "success", "message": "Dataset deleted successfully"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error deleting dataset: {str(e)}")

@app.get("/api/analyze-dataset/{dataset_id}")
async def analyze_dataset(dataset_id: str):
    """Analyze a specific dataset using AI"""
    try:
        # Find the dataset file
        data_folder = Path(DATA_FOLDER_PATH)
        dataset_file = None
        
        for file_path in data_folder.iterdir():
            if dataset_id in file_path.name:
                dataset_file = file_path
                break
        
        if not dataset_file:
            raise HTTPException(status_code=404, detail="Dataset not found")
        
        # Generate AI analysis
        analysis_response = ai_model.generate_response(
            f"Analyze the dataset: {dataset_file.name}",
            {"dataset_id": dataset_id, "analysis_request": True}
        )
        
        return {
            "dataset_id": dataset_id,
            "analysis": analysis_response.response,
            "insights": analysis_response.insights,
            "suggestions": analysis_response.suggestions
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Analysis error: {str(e)}")

@app.get("/api/operation-data/{operation_type}")
async def get_operation_data(operation_type: str):
    """Get KPIs and chart data for a specific operation type"""
    try:
        # Get datasets for this operation type
        datasets_response = await get_datasets(operation_type)
        datasets = datasets_response["datasets"]
        
        # Calculate KPIs from real data or provide defaults
        kpis = calculate_kpis_from_data(datasets, operation_type)
        
        # Generate chart data
        chart_data = generate_chart_data_from_datasets(datasets, operation_type)
        
        return {
            "operation_type": operation_type,
            "kpis": kpis,
            "chart_data": chart_data,
            "datasets_count": len(datasets),
            "last_updated": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting operation data: {str(e)}")

def calculate_kpis_from_data(datasets: List[Dict], operation_type: str) -> Dict[str, Any]:
    """Calculate KPIs from uploaded datasets or return realistic defaults"""
    
    # Default KPIs based on operation type
    default_kpis = {
        "terminal": {
            "efficiency": 87.5,
            "activeUnits": 24,
            "uptime": 96.2,
            "alerts": 3,
            "throughput": 1250,
            "errorRate": 2.1,
            "avgProcessingTime": 4.3,
            "costSavings": 125000
        },
        "courier": {
            "efficiency": 91.2,
            "activeUnits": 18,
            "uptime": 94.8,
            "alerts": 2,
            "throughput": 890,
            "errorRate": 1.8,
            "avgProcessingTime": 3.7,
            "costSavings": 89000
        },
        "workforce": {
            "efficiency": 89.1,
            "activeUnits": 156,
            "uptime": 97.5,
            "alerts": 5,
            "throughput": 2100,
            "errorRate": 1.2,
            "avgProcessingTime": 2.8,
            "costSavings": 234000
        },
        "energy": {
            "efficiency": 92.8,
            "activeUnits": 12,
            "uptime": 98.9,
            "alerts": 1,
            "throughput": 3450,
            "errorRate": 0.8,
            "avgProcessingTime": 1.9,
            "costSavings": 456000
        }
    }
    
    if not datasets or not PANDAS_AVAILABLE:
        return default_kpis.get(operation_type, default_kpis["terminal"])
    
    # TODO: Implement real KPI calculation from datasets
    # For now, return defaults with slight variations based on data
    kpis = default_kpis.get(operation_type, default_kpis["terminal"])
    
    # Add some variation based on number of datasets
    variation_factor = min(1.1, 1.0 + (len(datasets) * 0.02))
    kpis["efficiency"] = round(kpis["efficiency"] * variation_factor, 1)
    kpis["throughput"] = int(kpis["throughput"] * variation_factor)
    
    return kpis

def generate_chart_data_from_datasets(datasets: List[Dict], operation_type: str) -> Dict[str, List]:
    """Generate chart data from uploaded datasets or return sample data"""
    
    # Sample chart data for different operation types
    sample_data = {
        "terminal": {
            "line_chart": [
                {"name": "Jan", "value": 400, "efficiency": 85},
                {"name": "Feb", "value": 300, "efficiency": 87},
                {"name": "Mar", "value": 500, "efficiency": 89},
                {"name": "Apr", "value": 450, "efficiency": 91},
                {"name": "May", "value": 600, "efficiency": 88},
                {"name": "Jun", "value": 550, "efficiency": 92}
            ],
            "bar_chart": [
                {"category": "Processing", "value": 65},
                {"category": "Waiting", "value": 20},
                {"category": "Maintenance", "value": 10},
                {"category": "Idle", "value": 5}
            ],
            "pie_chart": [
                {"name": "Active", "value": 75, "color": "#22c55e"},
                {"name": "Maintenance", "value": 15, "color": "#f59e0b"},
                {"name": "Offline", "value": 10, "color": "#ef4444"}
            ]
        }
    }
    
    # TODO: Implement real data processing from uploaded files
    # For now, return sample data with operation type variations
    
    base_data = sample_data.get("terminal")
    if operation_type == "courier":
        # Modify data for courier operations
        base_data["line_chart"] = [
            {**item, "value": int(item["value"] * 0.7)} for item in base_data["line_chart"]
        ]
    elif operation_type == "workforce":
        # Modify data for workforce operations
        base_data["line_chart"] = [
            {**item, "value": int(item["value"] * 1.4)} for item in base_data["line_chart"]
        ]
    elif operation_type == "energy":
        # Modify data for energy operations
        base_data["line_chart"] = [
            {**item, "value": int(item["value"] * 2.1)} for item in base_data["line_chart"]
        ]
    
    return base_data

if __name__ == "__main__":
    print("üöÄ Starting Honeywell Terminal Manager API...")
    print(f"üìÅ Data folder: {DATA_FOLDER_PATH}")
    print(f"ü§ñ AI Model: {MODEL_FILE_PATH}")
    print(f"üîó Model loaded: {ai_model.model is not None}")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8001,
        reload=True
    )